import pandas as pd
import requests
from io import StringIO
import os
import numpy as np
import re
from dateutil import parser
import glob
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import hashlib
import warnings
import base64
warnings.filterwarnings('ignore')

def debug_constituents_issue():
    """Funci√≥n de debug para diagnosticar el problema"""
    print("\n=== DEBUG: DIAGN√ìSTICO DE CONSTITUYENTES ===")
    
    # 1. Verificar archivos CSV de datos
    data_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
    ticker_files = [f for f in data_files if not f.endswith('_changes.csv')]
    print(f"üìÅ Archivos de datos encontrados: {len(ticker_files)}")
    if ticker_files:
        print(f"   Primeros 10: {[os.path.basename(f) for f in ticker_files[:10]]}")
    
    # 2. Verificar archivos de cambios
    change_files = {
        'sp500_changes.csv': os.path.exists('sp500_changes.csv'),
        'ndx_changes.csv': os.path.exists('ndx_changes.csv'),
        'data/sp500_changes.csv': os.path.exists('data/sp500_changes.csv'),
        'data/ndx_changes.csv': os.path.exists('data/ndx_changes.csv')
    }
    print(f"\nüìã Archivos de cambios hist√≥ricos:")
    for file, exists in change_files.items():
        print(f"   {file}: {'‚úÖ' if exists else '‚ùå'}")
    
    # 3. Probar obtenci√≥n de constituyentes actuales
    print(f"\nüîç Probando obtenci√≥n de constituyentes actuales:")
    
    # SP500
    try:
        sp500_current = get_sp500_tickers_from_wikipedia()
        print(f"   S&P 500: {len(sp500_current.get('tickers', []))} tickers")
        if sp500_current.get('tickers'):
            print(f"   Primeros 5: {sp500_current['tickers'][:5]}")
    except Exception as e:
        print(f"   S&P 500: ‚ùå Error - {e}")
    
    # NDX
    try:
        ndx_current = get_nasdaq100_tickers_from_wikipedia()
        print(f"   NASDAQ-100: {len(ndx_current.get('tickers', []))} tickers")
        if ndx_current.get('tickers'):
            print(f"   Primeros 5: {ndx_current['tickers'][:5]}")
    except Exception as e:
        print(f"   NASDAQ-100: ‚ùå Error - {e}")
    
    print("=== FIN DEBUG ===\n")

# Directorios
DATA_DIR = "data"
CACHE_DIR = os.path.join(DATA_DIR, "cache")
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

# Cache global mejorado
_historical_cache = {}
_constituents_cache = {}

def get_cache_key(*args):
    """Genera clave de cach√© √∫nica"""
    return hashlib.md5(str(args).encode()).hexdigest()

def save_cache(key, data, prefix="cache"):
    """Guarda datos en cach√© persistente"""
    try:
        cache_file = os.path.join(CACHE_DIR, f"{prefix}_{key}.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
    except Exception as e:
        print(f"Error guardando cach√©: {e}")

def load_cache(key, prefix="cache", max_age_days=7):
    """Carga datos de cach√© si no son muy antiguos"""
    try:
        cache_file = os.path.join(CACHE_DIR, f"{prefix}_{key}.pkl")
        if os.path.exists(cache_file):
            # Verificar antig√ºedad
            file_age = (datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_file))).days
            if file_age <= max_age_days:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
    except Exception as e:
        print(f"Error cargando cach√©: {e}")
    return None

def parse_wikipedia_date(date_str):
    """Parsea fechas de Wikipedia"""
    if pd.isna(date_str) or not date_str or str(date_str).lower() in ['nan', 'none', '']:
        return None
    
    date_str = str(date_str).strip()
    
    try:
        parsed_date = parser.parse(date_str, fuzzy=True)
        return parsed_date.date()
    except:
        try:
            month_map = {
                'january': 1, 'february': 2, 'march': 3, 'april': 4,
                'may': 5, 'june': 6, 'july': 7, 'august': 8,
                'september': 9, 'october': 10, 'november': 11, 'december': 12,
                'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,
                'jun': 6, 'jul': 7, 'aug': 8, 'sep': 9,
                'oct': 10, 'nov': 11, 'dec': 12
            }
            
            clean_date = re.sub(r'[^\w\s,]', ' ', date_str.lower())
            parts = clean_date.split()
            
            if len(parts) >= 3:
                month = None
                for part in parts:
                    if part.replace(',', '') in month_map:
                        month = month_map[part.replace(',', '')]
                        break
                
                numbers = [int(re.findall(r'\d+', part)[0]) for part in parts if re.findall(r'\d+', part)]
                
                if month and len(numbers) >= 2:
                    day = min(numbers)
                    year = max(numbers)
                    
                    if day <= 31 and year >= 1900:
                        return date(year, month, day)
            
            return None
        except:
            return None

def get_sp500_tickers_from_wikipedia():
    """Obtiene los tickers actuales del S&P 500"""
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        tables = pd.read_html(url)
        df = tables[0]
        tickers = df['Symbol'].str.replace('.', '-').tolist()
        print(f"‚úÖ Obtenidos {len(tickers)} tickers del S&P 500")
        return {'tickers': tickers}
    except Exception as e:
        print(f"‚ùå Error obteniendo S&P 500: {e}")
        return {'tickers': []}

def get_nasdaq100_tickers_from_wikipedia():
    """Obtiene los tickers actuales del NASDAQ-100"""
    try:
        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        tables = pd.read_html(url)
        # La tabla puede estar en diferentes posiciones, intentar varias
        for i in [4, 3, 2, 1]:
            try:
                df = tables[i]
                if 'Ticker' in df.columns or 'Symbol' in df.columns:
                    col_name = 'Ticker' if 'Ticker' in df.columns else 'Symbol'
                    tickers = df[col_name].str.replace('.', '-').tolist()
                    print(f"‚úÖ Obtenidos {len(tickers)} tickers del NASDAQ-100")
                    return {'tickers': tickers}
            except:
                continue
        # Si no encuentra la tabla correcta, usar una lista conocida
        print("‚ö†Ô∏è No se pudo encontrar la tabla del NASDAQ-100, usando lista de respaldo")
        return {'tickers': []}
    except Exception as e:
        print(f"‚ùå Error obteniendo NASDAQ-100: {e}")
        return {'tickers': []}

def get_sp500_historical_changes():
    """Obtiene los cambios hist√≥ricos del S&P 500 desde CSV local"""
    local_csv_path = "sp500_changes.csv"
    fallback_path = "data/sp500_changes.csv"
    
    changes_df = pd.DataFrame()
    
    for csv_path in [local_csv_path, fallback_path]:
        if os.path.exists(csv_path):
            try:
                print(f"üìÇ Cargando cambios hist√≥ricos S&P 500 desde {csv_path}...")
                changes_df = pd.read_csv(csv_path, parse_dates=['Date'])
                print(f"‚úÖ Cargados {len(changes_df)} cambios hist√≥ricos del S&P 500")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Error leyendo {csv_path}: {e}")
    
    if changes_df.empty:
        print("‚ùå No se pudieron obtener cambios hist√≥ricos del S&P 500")
    else:
        # Asegurarse de que las fechas est√©n en formato datetime
        changes_df['Date'] = pd.to_datetime(changes_df['Date'])
        # Ordenar por fecha descendente
        changes_df = changes_df.sort_values('Date', ascending=False)
        
        if not changes_df.empty:
            date_range = f"{changes_df['Date'].min().date()} a {changes_df['Date'].max().date()}"
            print(f"üìÖ Rango de cambios S&P 500: {date_range}")
    
    return changes_df

def get_nasdaq100_historical_changes():
    """Obtiene los cambios hist√≥ricos del NASDAQ-100 desde CSV local"""
    local_csv_path = "ndx_changes.csv"
    fallback_path = "data/ndx_changes.csv"
    
    changes_df = pd.DataFrame()
    
    for csv_path in [local_csv_path, fallback_path]:
        if os.path.exists(csv_path):
            try:
                print(f"üìÇ Cargando cambios hist√≥ricos NASDAQ-100 desde {csv_path}...")
                changes_df = pd.read_csv(csv_path, parse_dates=['Date'])
                print(f"‚úÖ Cargados {len(changes_df)} cambios hist√≥ricos del NASDAQ-100")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Error leyendo {csv_path}: {e}")
    
    if changes_df.empty:
        print("‚ùå No se pudieron obtener cambios hist√≥ricos del NASDAQ-100")
    else:
        # Asegurarse de que las fechas est√©n en formato datetime
        changes_df['Date'] = pd.to_datetime(changes_df['Date'])
        # Ordenar por fecha descendente
        changes_df = changes_df.sort_values('Date', ascending=False)
        
        if not changes_df.empty:
            date_range = f"{changes_df['Date'].min().date()} a {changes_df['Date'].max().date()}"
            print(f"üìÖ Rango de cambios NASDAQ-100: {date_range}")
    
    return changes_df

def get_current_constituents(index_name):
    """Obtiene constituyentes actuales con cach√©"""
    cache_key = f"current_{index_name}"
    
    cached_data = load_cache(cache_key, prefix="constituents", max_age_days=1)
    if cached_data is not None:
        return cached_data
    
    if index_name == "SP500":
        result = get_sp500_tickers_from_wikipedia()
    elif index_name == "NDX":
        result = get_nasdaq100_tickers_from_wikipedia()
    elif index_name == "Ambos (SP500 + NDX)":
        # Combinar ambos √≠ndices
        sp500 = get_sp500_tickers_from_wikipedia()
        ndx = get_nasdaq100_tickers_from_wikipedia()
        # Unir los tickers de ambos √≠ndices (sin duplicados)
        combined_tickers = list(set(sp500['tickers'] + ndx['tickers']))
        result = {'tickers': combined_tickers}
        print(f"‚úÖ Combinados: {len(sp500['tickers'])} S&P500 + {len(ndx['tickers'])} NDX = {len(combined_tickers)} √∫nicos")
    else:
        # En lugar de lanzar error, intentar interpretar
        print(f"‚ö†Ô∏è √çndice '{index_name}' no reconocido, intentando interpretar...")
        if "SP500" in index_name or "S&P" in index_name:
            result = get_sp500_tickers_from_wikipedia()
        elif "NDX" in index_name or "NASDAQ" in index_name:
            result = get_nasdaq100_tickers_from_wikipedia()
        else:
            print(f"‚ùå No se pudo interpretar el √≠ndice '{index_name}'")
            return {'tickers': []}
    
    save_cache(cache_key, result, prefix="constituents")
    return result

def get_all_available_tickers_with_historical_validation(index_name, start_date, end_date):
    """
    Obtiene los tickers que realmente estaban en el √≠ndice durante el per√≠odo especificado
    VERSI√ìN CORREGIDA PARA MANEJAR FECHAS FUTURAS
    """
    try:
        print(f"üîç Validando constituyentes hist√≥ricos de {index_name} para {start_date} a {end_date}")
        
        # 1. Obtener constituyentes actuales
        if index_name == "SP500":
            current_constituents = get_sp500_tickers_from_wikipedia()
        elif index_name == "NDX":
            current_constituents = get_nasdaq100_tickers_from_wikipedia()
        elif index_name == "Ambos (SP500 + NDX)":
            sp500_current = get_sp500_tickers_from_wikipedia()
            ndx_current = get_nasdaq100_tickers_from_wikipedia()
            current_constituents = {
                'tickers': list(set(sp500_current['tickers'] + ndx_current['tickers']))
            }
        else:
            print(f"‚ùå √çndice no soportado: {index_name}")
            return get_current_constituents(index_name), None
        
        if not current_constituents or 'tickers' not in current_constituents:
            print("‚ùå No se pudieron obtener constituyentes actuales")
            return {'tickers': [], 'data': [], 'historical_data_available': False}, None
        
        print(f"‚úÖ Constituyentes actuales: {len(current_constituents['tickers'])}")
        
        # 2. Obtener cambios hist√≥ricos
        if index_name == "SP500":
            historical_changes = get_sp500_historical_changes()
        elif index_name == "NDX":
            historical_changes = get_nasdaq100_historical_changes()
        else:  # Ambos
            sp500_changes = get_sp500_historical_changes()
            ndx_changes = get_nasdaq100_historical_changes()
            if not sp500_changes.empty and not ndx_changes.empty:
                historical_changes = pd.concat([sp500_changes, ndx_changes], ignore_index=True)
                historical_changes = historical_changes.drop_duplicates(subset=['Date', 'Ticker', 'Action'])
            elif not sp500_changes.empty:
                historical_changes = sp500_changes
            else:
                historical_changes = ndx_changes
        
        # 3. Convertir fechas a datetime
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        elif isinstance(start_date, date):
            start_date = pd.to_datetime(start_date)
            
        if isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)
        elif isinstance(end_date, date):
            end_date = pd.to_datetime(end_date)
        
        print(f"üìÖ Per√≠odo de validaci√≥n: {start_date.date()} a {end_date.date()}")
        
        # 4. Empezar con constituyentes actuales
        valid_tickers = set(current_constituents['tickers'])
        print(f"üéØ Tickers iniciales (constituyentes actuales): {len(valid_tickers)}")
        
        # 5. Si tenemos datos hist√≥ricos, aplicar correcciones
        if not historical_changes.empty:
            print(f"üìä Procesando {len(historical_changes)} cambios hist√≥ricos...")
            
            # Obtener fecha actual real
            today = pd.to_datetime(datetime.now().date())
            
            # Filtrar cambios futuros (posteriores a hoy)
            future_changes = historical_changes[pd.to_datetime(historical_changes['Date']) > today]
            if not future_changes.empty:
                print(f"‚ö†Ô∏è Ignorando {len(future_changes)} cambios con fechas futuras")
            
            # Solo procesar cambios hasta hoy
            historical_changes = historical_changes[pd.to_datetime(historical_changes['Date']) <= today]
            
            # Para cada cambio hist√≥rico, ver si afecta nuestro per√≠odo
            for _, change in historical_changes.iterrows():
                change_date = pd.to_datetime(change['Date'])
                ticker = str(change['Ticker']).upper()
                action = change['Action']
                
                # Si el cambio ocurri√≥ despu√©s del per√≠odo de backtest, ignorarlo
                if change_date > end_date:
                    continue
                
                # L√≥gica corregida:
                if action == 'Added':
                    # Si fue a√±adido DESPU√âS del inicio del per√≠odo, no estaba al principio
                    if change_date > start_date:
                        valid_tickers.discard(ticker)
                    # Si fue a√±adido ANTES del inicio, ya est√° en valid_tickers (OK)
                    
                elif action == 'Removed':
                    # Si fue removido ANTES del inicio del per√≠odo, no estaba presente
                    if change_date <= start_date:
                        valid_tickers.discard(ticker)
                    # Si fue removido DURANTE el per√≠odo, estaba presente al inicio
                    # pero debemos mantenerlo para el backtest
        
        print(f"üìä Tickers v√°lidos hist√≥ricamente: {len(valid_tickers)}")
        
        # 6. Verificar tickers con datos CSV disponibles
        csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
        available_csv_tickers = set()
        
        for csv_file in csv_files:
            filename = os.path.basename(csv_file)
            if filename.endswith('.csv') and filename != 'sp500_changes.csv' and filename != 'ndx_changes.csv':
                ticker = filename.replace('.csv', '').upper()
                available_csv_tickers.add(ticker)
        
        print(f"üìÅ Tickers con datos CSV disponibles: {len(available_csv_tickers)}")
        
        # 7. Intersecci√≥n: tickers v√°lidos Y con datos disponibles
        final_tickers = list(valid_tickers & available_csv_tickers)
        print(f"üìà Tickers finales (intersecci√≥n): {len(final_tickers)}")
        
        # 8. Verificaci√≥n adicional de tickers problem√°ticos conocidos
        problematic_tickers = ['RIG', 'OI', 'VNT']
        removed_problematic = []
        
        for ticker in problematic_tickers:
            if ticker in final_tickers:
                # Verificar si el ticker fue removido antes del per√≠odo
                if not historical_changes.empty:
                    removal_info = historical_changes[
                        (historical_changes['Ticker'] == ticker) & 
                        (historical_changes['Action'] == 'Removed')
                    ]
                    if not removal_info.empty:
                        removal_date = pd.to_datetime(removal_info.iloc[0]['Date'])
                        if removal_date <= start_date:
                            print(f"‚ö†Ô∏è Removiendo {ticker} - fue eliminado del √≠ndice el {removal_date.date()}")
                            final_tickers.remove(ticker)
                            removed_problematic.append(ticker)
        
        # Verificaci√≥n final
        if len(final_tickers) == 0:
            print("‚ö†Ô∏è No se encontraron tickers v√°lidos, usando constituyentes actuales como fallback")
            # Fallback: usar constituyentes actuales que tengan datos CSV
            final_tickers = list(set(current_constituents['tickers']) & available_csv_tickers)
            print(f"‚úÖ Fallback con {len(final_tickers)} tickers")
        
        print(f"‚úÖ Tickers v√°lidos finales: {len(final_tickers)}")
        
        if removed_problematic:
            print(f"üö´ Tickers problem√°ticos removidos: {removed_problematic}")
        
        return {
            'tickers': final_tickers,
            'data': [],
            'historical_data_available': True if len(historical_changes) > 0 else False,
            'removed_problematic': removed_problematic
        }, None
        
    except Exception as e:
        error_msg = f"Error en validaci√≥n hist√≥rica: {str(e)}"
        print(f"‚ùå {error_msg}")
        import traceback
        traceback.print_exc()
        
        # Fallback a constituyentes actuales
        try:
            print("üîÑ Intentando fallback a constituyentes actuales...")
            current = get_current_constituents(index_name)
            if current and 'tickers' in current and current['tickers']:
                # Verificar qu√© tickers tienen datos CSV
                csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
                available_csv_tickers = set()
                
                for csv_file in csv_files:
                    filename = os.path.basename(csv_file)
                    if filename.endswith('.csv') and filename != 'sp500_changes.csv' and filename != 'ndx_changes.csv':
                        ticker = filename.replace('.csv', '').upper()
                        available_csv_tickers.add(ticker)
                
                final_tickers = list(set(current['tickers']) & available_csv_tickers)
                
                print(f"‚úÖ Fallback exitoso con {len(final_tickers)} tickers")
                return {
                    'tickers': final_tickers,
                    'data': [],
                    'historical_data_available': False,
                    'note': f'Fallback due to error: {str(e)}'
                }, f"Warning: Using current constituents as fallback - {str(e)}"
            else:
                print("‚ùå Fallback tambi√©n fall√≥")
                return None, error_msg
        except Exception as fallback_error:
            print(f"‚ùå Error en fallback: {fallback_error}")
            return None, f"{error_msg} | Fallback error: {str(fallback_error)}"
            
def get_constituents_at_date(index_name, start_date, end_date):
    """Obtiene constituyentes con cach√© mejorado"""
    
    # A√ëADIR ESTA L√çNEA PARA DEBUG
    debug_constituents_issue()
    
    cache_key = get_cache_key(index_name, start_date, end_date)
    
    # Cach√© en memoria
    if cache_key in _constituents_cache:
        print(f"‚ö° Usando cach√© en memoria para {index_name}")
        return _constituents_cache[cache_key], None
    
    # Cach√© en disco
    cached_data = load_cache(cache_key, prefix="constituents", max_age_days=7)
    if cached_data is not None:
        print(f"üì¶ Usando cach√© en disco para {index_name}")
        _constituents_cache[cache_key] = cached_data
        return cached_data, None
    
    try:
        # Obtener datos frescos con validaci√≥n hist√≥rica
        result, error = get_all_available_tickers_with_historical_validation(
            index_name, start_date, end_date
        )
        
        if result and 'tickers' in result and result['tickers']:
            # Guardar en ambos cach√©s
            _constituents_cache[cache_key] = result
            save_cache(cache_key, result, prefix="constituents")
            print(f"‚úÖ Constituyentes obtenidos: {len(result['tickers'])} tickers")
            return result, error
        else:
            # Fallback
            print("‚ö†Ô∏è No se obtuvieron resultados, intentando fallback...")
            current_data = get_current_constituents(index_name)
            
            if current_data and 'tickers' in current_data and current_data['tickers']:
                # A√ëADIR VERIFICACI√ìN DE ARCHIVOS CSV
                csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
                available_tickers = set()
                for csv_file in csv_files:
                    filename = os.path.basename(csv_file)
                    if filename.endswith('.csv') and not filename.endswith('_changes.csv'):
                        ticker = filename.replace('.csv', '').upper()
                        available_tickers.add(ticker)
                
                # Intersecci√≥n con archivos disponibles
                valid_tickers = list(set(current_data['tickers']) & available_tickers)
                
                fallback_result = {
                    'tickers': valid_tickers,
                    'data': [],
                    'historical_data_available': False,
                    'note': 'Fallback to current constituents'
                }
                print(f"‚úÖ Usando fallback con {len(valid_tickers)} tickers actuales con datos")
                return fallback_result, "Warning: Using current constituents as fallback"
            else:
                print("‚ùå Fallback tambi√©n fall√≥")
                return None, "Error: No se pudieron obtener constituyentes"
            
    except Exception as e:
        error_msg = f"Error obteniendo constituyentes para {index_name}: {e}"
        print(f"‚ö†Ô∏è {error_msg}")
        import traceback
        traceback.print_exc()
        
        # Fallback mejorado
        try:
            print("üîÑ Intentando fallback a constituyentes actuales...")
            current_data = get_current_constituents(index_name)
            
            if current_data and 'tickers' in current_data and current_data['tickers']:
                # Verificar archivos CSV disponibles
                csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
                available_tickers = set()
                for csv_file in csv_files:
                    filename = os.path.basename(csv_file)
                    if filename.endswith('.csv') and not filename.endswith('_changes.csv'):
                        ticker = filename.replace('.csv', '').upper()
                        available_tickers.add(ticker)
                
                valid_tickers = list(set(current_data['tickers']) & available_tickers)
                
                fallback_result = {
                    'tickers': valid_tickers,
                    'data': [],
                    'historical_data_available': False,
                    'note': f'Fallback to current constituents due to: {str(e)}'
                }
                print(f"‚úÖ Fallback exitoso con {len(valid_tickers)} tickers")
                return fallback_result, f"Warning: Using current constituents as fallback - {str(e)}"
            else:
                print("‚ùå Fallback tambi√©n fall√≥")
                return None, error_msg
        except Exception as fallback_error:
            print(f"‚ùå Error en fallback: {fallback_error}")
            return None, f"{error_msg} | Fallback error: {str(fallback_error)}"

def load_prices_from_csv_parallel(tickers, start_date, end_date, load_full_data=True):
    """Carga precios desde CSV en PARALELO"""
    prices_data = {}
    ohlc_data = {}
    
    def load_single_ticker(ticker):
        csv_path = f"data/{ticker}.csv"
        if not os.path.exists(csv_path):
            return ticker, None, None
        
        try:
            df = pd.read_csv(csv_path, index_col="Date", parse_dates=True)
            
            if hasattr(df.index, 'tz') and df.index.tz is not None:
                df.index = df.index.tz_localize(None)
            
            start_filter = start_date.date() if isinstance(start_date, datetime) else start_date
            end_filter = end_date.date() if isinstance(end_date, datetime) else end_date
            
            df = df[(df.index.date >= start_filter) & (df.index.date <= end_filter)]
            
            if df.empty:
                return ticker, None, None
            
            # Usar Close ya que los datos est√°n ajustados
            if 'Close' in df.columns:
                price = df['Close']
            else:
                return ticker, None, None
            
            ohlc = None
            if load_full_data and all(col in df.columns for col in ['High', 'Low', 'Close']):
                ohlc = {
                    'High': df['High'],
                    'Low': df['Low'],
                    'Close': df['Close'],
                    'Volume': df['Volume'] if 'Volume' in df.columns else None
                }
            
            return ticker, price, ohlc
            
        except Exception as e:
            return ticker, None, None
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = [executor.submit(load_single_ticker, ticker) for ticker in tickers]
        
        for future in futures:
            ticker, price, ohlc = future.result()
            if price is not None:
                prices_data[ticker] = price
            if ohlc is not None:
                ohlc_data[ticker] = ohlc
    
    if prices_data:
        prices_df = pd.DataFrame(prices_data)
        prices_df = prices_df.fillna(method='ffill').fillna(method='bfill')
        return prices_df, ohlc_data
    else:
        return pd.DataFrame(), {}

def create_download_link(df, filename, link_text):
    """Crea un enlace de descarga para un DataFrame"""
    try:
        csv = df.to_csv(index=False)
        b64 = base64.b64encode(csv.encode()).decode()
        href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">{link_text}</a>'
        return href
    except Exception as e:
        print(f"Error creando enlace de descarga: {e}")
        return None

def generate_removed_tickers_summary():
    """Genera resumen de tickers removidos con cach√©"""
    cache_key = "removed_tickers_summary"
    
    cached_data = load_cache(cache_key, max_age_days=30)
    if cached_data is not None:
        return cached_data
    
    summary_data = []
    
    # Procesar S&P 500
    sp500_changes = get_sp500_historical_changes()
    if not sp500_changes.empty:
        removed = sp500_changes[sp500_changes['Action'] == 'Removed']
        for _, row in removed.iterrows():
            summary_data.append({
                'Index': 'S&P 500',
                'Ticker': row['Ticker'],
                'Date': row['Date'],
                'Action': 'Removed'
            })
    
    # Procesar NASDAQ-100
    ndx_changes = get_nasdaq100_historical_changes()
    if not ndx_changes.empty:
        removed = ndx_changes[ndx_changes['Action'] == 'Removed']
        for _, row in removed.iterrows():
            summary_data.append({
                'Index': 'NASDAQ-100',
                'Ticker': row['Ticker'],
                'Date': row['Date'],
                'Action': 'Removed'
            })
    
    if summary_data:
        summary = pd.DataFrame(summary_data)
        summary['Date'] = pd.to_datetime(summary['Date'])
        summary = summary.sort_values('Date', ascending=False)
        save_cache(cache_key, summary)
        return summary
    
    return pd.DataFrame()

def clear_cache():
    """Limpia toda la cach√©"""
    import shutil
    if os.path.exists(CACHE_DIR):
        shutil.rmtree(CACHE_DIR)
        os.makedirs(CACHE_DIR)
    print("‚úÖ Cach√© limpiado")

def get_cache_size():
    """Obtiene el tama√±o de la cach√© en MB"""
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(CACHE_DIR):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            total_size += os.path.getsize(fp)
    return total_size / (1024 * 1024)  # Convertir a MB

# Solo ejecutar si se llama directamente
if __name__ == "__main__":
    print(f"üíæ Tama√±o actual de cach√©: {get_cache_size():.2f} MB")
    
    test_indices = ["SP500", "NDX", "Ambos (SP500 + NDX)"]
    for idx in test_indices:
        print(f"\nüß™ Probando √≠ndice: {idx}")
        result = get_current_constituents(idx)
        if result and 'tickers' in result:
            print(f"‚úÖ {idx}: {len(result['tickers'])} tickers encontrados")
        else:
            print(f"‚ùå {idx}: Error obteniendo tickers")
    
    # Test de validaci√≥n hist√≥rica
    print("\nüß™ Probando validaci√≥n hist√≥rica...")
    test_date = datetime(2023, 8, 1)
    result, error = get_all_available_tickers_with_historical_validation(
        "SP500", test_date, test_date + timedelta(days=30)
    )
    
    if result:
        print(f"‚úÖ Validaci√≥n hist√≥rica: {len(result['tickers'])} tickers")
        if 'removed_problematic' in result:
            print(f"üö´ Tickers problem√°ticos removidos: {result['removed_problematic']}")
        
        # Verificar que RIG, OI, VNT no est√©n en los resultados
        problematic = ['RIG', 'OI', 'VNT']
        for ticker in problematic:
            if ticker in result['tickers']:
                print(f"‚ùå ERROR: {ticker} no deber√≠a estar en los resultados de 2023!")
            else:
                print(f"‚úÖ Correcto: {ticker} no est√° en los resultados")
