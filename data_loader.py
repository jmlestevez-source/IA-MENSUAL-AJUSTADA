import pandas as pd
import requests
from io import StringIO
import os
import numpy as np
import re
from dateutil import parser
import glob
from datetime import datetime, date, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import pickle
import hashlib
import warnings
import base64
warnings.filterwarnings('ignore')

# Directorios
DATA_DIR = "data"
CACHE_DIR = os.path.join(DATA_DIR, "cache")
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(CACHE_DIR, exist_ok=True)

# Cache global mejorado
_historical_cache = {}
_constituents_cache = {}

def get_cache_key(*args):
    """Genera clave de cach√© √∫nica"""
    return hashlib.md5(str(args).encode()).hexdigest()

def save_cache(key, data, prefix="cache"):
    """Guarda datos en cach√© persistente"""
    try:
        cache_file = os.path.join(CACHE_DIR, f"{prefix}_{key}.pkl")
        with open(cache_file, 'wb') as f:
            pickle.dump(data, f)
    except Exception as e:
        print(f"Error guardando cach√©: {e}")

def load_cache(key, prefix="cache", max_age_days=7):
    """Carga datos de cach√© si no son muy antiguos"""
    try:
        cache_file = os.path.join(CACHE_DIR, f"{prefix}_{key}.pkl")
        if os.path.exists(cache_file):
            # Verificar antig√ºedad
            file_age = (datetime.now() - datetime.fromtimestamp(os.path.getmtime(cache_file))).days
            if file_age <= max_age_days:
                with open(cache_file, 'rb') as f:
                    return pickle.load(f)
    except Exception as e:
        print(f"Error cargando cach√©: {e}")
    return None

def parse_wikipedia_date(date_str):
    """Parsea fechas de Wikipedia"""
    if pd.isna(date_str) or not date_str or str(date_str).lower() in ['nan', 'none', '']:
        return None
    
    date_str = str(date_str).strip()
    
    try:
        parsed_date = parser.parse(date_str, fuzzy=True)
        return parsed_date.date()
    except:
        try:
            month_map = {
                'january': 1, 'february': 2, 'march': 3, 'april': 4,
                'may': 5, 'june': 6, 'july': 7, 'august': 8,
                'september': 9, 'october': 10, 'november': 11, 'december': 12,
                'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4,
                'jun': 6, 'jul': 7, 'aug': 8, 'sep': 9,
                'oct': 10, 'nov': 11, 'dec': 12
            }
            
            clean_date = re.sub(r'[^\w\s,]', ' ', date_str.lower())
            parts = clean_date.split()
            
            if len(parts) >= 3:
                month = None
                for part in parts:
                    if part.replace(',', '') in month_map:
                        month = month_map[part.replace(',', '')]
                        break
                
                numbers = [int(re.findall(r'\d+', part)[0]) for part in parts if re.findall(r'\d+', part)]
                
                if month and len(numbers) >= 2:
                    day = min(numbers)
                    year = max(numbers)
                    
                    if day <= 31 and year >= 1900:
                        return date(year, month, day)
            
            return None
        except:
            return None

def get_sp500_tickers_from_wikipedia():
    """Obtiene los tickers actuales del S&P 500"""
    try:
        url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        tables = pd.read_html(url)
        df = tables[0]
        tickers = df['Symbol'].str.replace('.', '-').tolist()
        print(f"‚úÖ Obtenidos {len(tickers)} tickers del S&P 500")
        return {'tickers': tickers}
    except Exception as e:
        print(f"‚ùå Error obteniendo S&P 500: {e}")
        return {'tickers': []}

def get_nasdaq100_tickers_from_wikipedia():
    """Obtiene los tickers actuales del NASDAQ-100"""
    try:
        url = "https://en.wikipedia.org/wiki/Nasdaq-100"
        tables = pd.read_html(url)
        # La tabla puede estar en diferentes posiciones, intentar varias
        for i in [4, 3, 2, 1]:
            try:
                df = tables[i]
                if 'Ticker' in df.columns or 'Symbol' in df.columns:
                    col_name = 'Ticker' if 'Ticker' in df.columns else 'Symbol'
                    tickers = df[col_name].str.replace('.', '-').tolist()
                    print(f"‚úÖ Obtenidos {len(tickers)} tickers del NASDAQ-100")
                    return {'tickers': tickers}
            except:
                continue
        # Si no encuentra la tabla correcta, usar una lista conocida
        print("‚ö†Ô∏è No se pudo encontrar la tabla del NASDAQ-100, usando lista de respaldo")
        return {'tickers': []}
    except Exception as e:
        print(f"‚ùå Error obteniendo NASDAQ-100: {e}")
        return {'tickers': []}

def get_sp500_historical_changes():
    """Obtiene los cambios hist√≥ricos del S&P 500 desde CSV local"""
    local_csv_path = "sp500_changes.csv"
    fallback_path = "data/sp500_changes.csv"
    
    changes_df = pd.DataFrame()
    
    for csv_path in [local_csv_path, fallback_path]:
        if os.path.exists(csv_path):
            try:
                print(f"üìÇ Cargando cambios hist√≥ricos S&P 500 desde {csv_path}...")
                changes_df = pd.read_csv(csv_path, parse_dates=['Date'])
                print(f"‚úÖ Cargados {len(changes_df)} cambios hist√≥ricos del S&P 500")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Error leyendo {csv_path}: {e}")
    
    if changes_df.empty:
        print("‚ùå No se pudieron obtener cambios hist√≥ricos del S&P 500")
    else:
        # Asegurarse de que las fechas est√©n en formato datetime
        changes_df['Date'] = pd.to_datetime(changes_df['Date'])
        # Ordenar por fecha descendente
        changes_df = changes_df.sort_values('Date', ascending=False)
        
        if not changes_df.empty:
            date_range = f"{changes_df['Date'].min().date()} a {changes_df['Date'].max().date()}"
            print(f"üìÖ Rango de cambios S&P 500: {date_range}")
    
    return changes_df

def get_nasdaq100_historical_changes():
    """Obtiene los cambios hist√≥ricos del NASDAQ-100 desde CSV local"""
    local_csv_path = "ndx_changes.csv"
    fallback_path = "data/ndx_changes.csv"
    
    changes_df = pd.DataFrame()
    
    for csv_path in [local_csv_path, fallback_path]:
        if os.path.exists(csv_path):
            try:
                print(f"üìÇ Cargando cambios hist√≥ricos NASDAQ-100 desde {csv_path}...")
                changes_df = pd.read_csv(csv_path, parse_dates=['Date'])
                print(f"‚úÖ Cargados {len(changes_df)} cambios hist√≥ricos del NASDAQ-100")
                break
            except Exception as e:
                print(f"‚ö†Ô∏è Error leyendo {csv_path}: {e}")
    
    if changes_df.empty:
        print("‚ùå No se pudieron obtener cambios hist√≥ricos del NASDAQ-100")
    else:
        # Asegurarse de que las fechas est√©n en formato datetime
        changes_df['Date'] = pd.to_datetime(changes_df['Date'])
        # Ordenar por fecha descendente
        changes_df = changes_df.sort_values('Date', ascending=False)
        
        if not changes_df.empty:
            date_range = f"{changes_df['Date'].min().date()} a {changes_df['Date'].max().date()}"
            print(f"üìÖ Rango de cambios NASDAQ-100: {date_range}")
    
    return changes_df

def get_current_constituents(index_name):
    """Obtiene constituyentes actuales con cach√©"""
    cache_key = f"current_{index_name}"
    
    cached_data = load_cache(cache_key, prefix="constituents", max_age_days=1)
    if cached_data is not None:
        return cached_data
    
    if index_name == "SP500":
        result = get_sp500_tickers_from_wikipedia()
    elif index_name == "NDX":
        result = get_nasdaq100_tickers_from_wikipedia()
    elif index_name == "Ambos (SP500 + NDX)":
        # Combinar ambos √≠ndices
        sp500 = get_sp500_tickers_from_wikipedia()
        ndx = get_nasdaq100_tickers_from_wikipedia()
        # Unir los tickers de ambos √≠ndices (sin duplicados)
        combined_tickers = list(set(sp500['tickers'] + ndx['tickers']))
        result = {'tickers': combined_tickers}
        print(f"‚úÖ Combinados: {len(sp500['tickers'])} S&P500 + {len(ndx['tickers'])} NDX = {len(combined_tickers)} √∫nicos")
    else:
        # En lugar de lanzar error, intentar interpretar
        print(f"‚ö†Ô∏è √çndice '{index_name}' no reconocido, intentando interpretar...")
        if "SP500" in index_name or "S&P" in index_name:
            result = get_sp500_tickers_from_wikipedia()
        elif "NDX" in index_name or "NASDAQ" in index_name:
            result = get_nasdaq100_tickers_from_wikipedia()
        else:
            print(f"‚ùå No se pudo interpretar el √≠ndice '{index_name}'")
            return {'tickers': []}
    
    save_cache(cache_key, result, prefix="constituents")
    return result

def get_all_available_tickers_with_historical_validation(index_name, start_date, end_date):
    """
    Obtiene los tickers que realmente estaban en el √≠ndice durante el per√≠odo especificado
    VERSI√ìN SIMPLIFICADA Y ROBUSTA
    """
    try:
        print(f"üîç Validando constituyentes hist√≥ricos de {index_name} para {start_date} a {end_date}")
        
        # 1. Obtener constituyentes actuales y cambios hist√≥ricos
        if index_name == "SP500":
            current_constituents = get_sp500_tickers_from_wikipedia()
            historical_changes = get_sp500_historical_changes()
        elif index_name == "NDX":
            current_constituents = get_nasdaq100_tickers_from_wikipedia()
            historical_changes = get_nasdaq100_historical_changes()
        elif index_name == "Ambos (SP500 + NDX)":
            # Combinar ambos √≠ndices
            sp500_current = get_sp500_tickers_from_wikipedia()
            ndx_current = get_nasdaq100_tickers_from_wikipedia()
            current_constituents = {
                'tickers': list(set(sp500_current['tickers'] + ndx_current['tickers']))
            }
            
            sp500_changes = get_sp500_historical_changes()
            ndx_changes = get_nasdaq100_historical_changes()
            
            # Combinar cambios hist√≥ricos
            if not sp500_changes.empty and not ndx_changes.empty:
                historical_changes = pd.concat([sp500_changes, ndx_changes], ignore_index=True)
                historical_changes = historical_changes.drop_duplicates(subset=['Date', 'Ticker', 'Action'])
            elif not sp500_changes.empty:
                historical_changes = sp500_changes
            else:
                historical_changes = ndx_changes
        else:
            print(f"‚ùå √çndice no soportado: {index_name}")
            return get_current_constituents(index_name), None
        
        # 2. Si no hay datos hist√≥ricos, usar solo constituyentes actuales
        if historical_changes.empty:
            print("‚ö†Ô∏è No hay cambios hist√≥ricos disponibles, usando solo constituyentes actuales")
            return {
                'tickers': current_constituents['tickers'],
                'data': [],
                'historical_data_available': False,
                'note': 'No historical data available'
            }, None
        
        # 3. Convertir fechas a datetime para comparaci√≥n
        if isinstance(start_date, str):
            start_date = pd.to_datetime(start_date)
        elif isinstance(start_date, date):
            start_date = pd.to_datetime(start_date)
            
        if isinstance(end_date, str):
            end_date = pd.to_datetime(end_date)
        elif isinstance(end_date, date):
            end_date = pd.to_datetime(end_date)
        
        print(f"üìÖ Per√≠odo de validaci√≥n: {start_date.date()} a {end_date.date()}")
        
        # 4. L√ìGICA SIMPLIFICADA: Solo filtrar los que fueron removidos ANTES del start_date
        valid_tickers = set(current_constituents['tickers'])
        
        # Agregar tickers que fueron removidos DESPU√âS del start_date (estaban presentes durante el per√≠odo)
        for _, change in historical_changes.iterrows():
            change_date = pd.to_datetime(change['Date'])
            ticker = str(change['Ticker']).upper()
            action = change['Action']
            
            if action == 'Removed' and change_date > start_date:
                # Si fue removido despu√©s del start_date, S√ç estaba presente durante el backtest
                valid_tickers.add(ticker)
            elif action == 'Removed' and change_date <= start_date:
                # Si fue removido antes o en el start_date, NO estaba presente
                valid_tickers.discard(ticker)
        
        print(f"üìä Tickers v√°lidos hist√≥ricamente: {len(valid_tickers)}")
        
        # 5. Filtrar solo los tickers que tienen datos CSV disponibles
        csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
        available_csv_tickers = set()
        
        for csv_file in csv_files:
            filename = os.path.basename(csv_file)
            if filename.endswith('.csv'):
                ticker = filename.replace('.csv', '').upper()
                available_csv_tickers.add(ticker)
        
        print(f"üìÅ Tickers con datos CSV disponibles: {len(available_csv_tickers)}")
        
        # Intersecci√≥n: tickers v√°lidos hist√≥ricamente Y con datos disponibles
        final_tickers = list(valid_tickers & available_csv_tickers)
        
        # 6. Verificar espec√≠ficamente los tickers problem√°ticos
        problematic_tickers = ['RIG', 'OI', 'VNT']
        removed_problematic = []
        
        for ticker in problematic_tickers:
            if ticker in final_tickers:
                # Verificar cu√°ndo fue removido
                removal_info = historical_changes[
                    (historical_changes['Ticker'] == ticker) & 
                    (historical_changes['Action'] == 'Removed')
                ]
                if not removal_info.empty:
                    removal_date = pd.to_datetime(removal_info.iloc[0]['Date'])
                    if removal_date <= start_date:
                        print(f"‚ö†Ô∏è REMOVIENDO {ticker} - fue eliminado del √≠ndice el {removal_date.date()}")
                        final_tickers.remove(ticker)
                        removed_problematic.append(ticker)
        
        print(f"üìà Tickers v√°lidos finales: {len(final_tickers)}")
        
        if removed_problematic:
            print(f"üö´ Tickers problem√°ticos removidos: {removed_problematic}")
        
        return {
            'tickers': final_tickers,
            'data': [],
            'historical_data_available': True,
            'removed_problematic': removed_problematic
        }, None
        
    except Exception as e:
        error_msg = f"Error en validaci√≥n hist√≥rica: {str(e)}"
        print(f"‚ùå {error_msg}")
        import traceback
        traceback.print_exc()
        
        # Fallback a constituyentes actuales
        try:
            print("üîÑ Intentando fallback a constituyentes actuales...")
            current = get_current_constituents(index_name)
            if current and 'tickers' in current and current['tickers']:
                print(f"‚úÖ Fallback exitoso con {len(current['tickers'])} tickers")
                return {
                    'tickers': current['tickers'],
                    'data': [],
                    'historical_data_available': False,
                    'note': f'Fallback due to error: {str(e)}'
                }, f"Warning: Using current constituents as fallback - {str(e)}"
            else:
                print("‚ùå Fallback tambi√©n fall√≥")
                return None, error_msg
        except Exception as fallback_error:
            print(f"‚ùå Error en fallback: {fallback_error}")
            return None, f"{error_msg} | Fallback error: {str(fallback_error)}"

def get_constituents_at_date(index_name, start_date, end_date):
    """Obtiene constituyentes con cach√© mejorado"""
    cache_key = get_cache_key(index_name, start_date, end_date)
    
    # Cach√© en memoria
    if cache_key in _constituents_cache:
        print(f"‚ö° Usando cach√© en memoria para {index_name}")
        return _constituents_cache[cache_key], None
    
    # Cach√© en disco
    cached_data = load_cache(cache_key, prefix="constituents", max_age_days=7)
    if cached_data is not None:
        print(f"üì¶ Usando cach√© en disco para {index_name}")
        _constituents_cache[cache_key] = cached_data
        return cached_data, None
    
    try:
        # Obtener datos frescos con validaci√≥n hist√≥rica
        result, error = get_all_available_tickers_with_historical_validation(
            index_name, start_date, end_date
        )
        
        if result and 'tickers' in result and result['tickers']:
            # Guardar en ambos cach√©s
            _constituents_cache[cache_key] = result
            save_cache(cache_key, result, prefix="constituents")
            print(f"‚úÖ Constituyentes obtenidos: {len(result['tickers'])} tickers")
            return result, error
        else:
            # Fallback
            print("‚ö†Ô∏è No se obtuvieron resultados, intentando fallback...")
            current_data = get_current_constituents(index_name)
            
            if current_data and 'tickers' in current_data and current_data['tickers']:
                fallback_result = {
                    'tickers': current_data['tickers'],
                    'data': [],
                    'historical_data_available': False,
                    'note': 'Fallback to current constituents'
                }
                print(f"‚úÖ Usando fallback con {len(current_data['tickers'])} tickers actuales")
                return fallback_result, "Warning: Using current constituents as fallback"
            else:
                print("‚ùå Fallback tambi√©n fall√≥")
                return None, "Error: No se pudieron obtener constituyentes"
            
    except Exception as e:
        error_msg = f"Error obteniendo constituyentes para {index_name}: {e}"
        print(f"‚ö†Ô∏è {error_msg}")
        
        # Fallback
        try:
            print("üîÑ Intentando fallback a constituyentes actuales...")
            current_data = get_current_constituents(index_name)
            
            if current_data and 'tickers' in current_data and current_data['tickers']:
                fallback_result = {
                    'tickers': current_data['tickers'],
                    'data': [],
                    'historical_data_available': False,
                    'note': f'Fallback to current constituents due to: {str(e)}'
                }
                print(f"‚úÖ Fallback exitoso con {len(current_data['tickers'])} tickers")
                return fallback_result, f"Warning: Using current constituents as fallback - {str(e)}"
            else:
                print("‚ùå Fallback tambi√©n fall√≥")
                return None, error_msg
        except Exception as fallback_error:
            print(f"‚ùå Error en fallback: {fallback_error}")
            return None, f"{error_msg} | Fallback error: {str(fallback_error)}"

def load_prices_from_csv_parallel(tickers, start_date, end_date, load_full_data=True):
    """Carga precios desde CSV en PARALELO"""
    prices_data = {}
    ohlc_data = {}
    
    def load_single_ticker(ticker):
        csv_path = f"data/{ticker}.csv"
        if not os.path.exists(csv_path):
            return ticker, None, None
        
        try:
            df = pd.read_csv(csv_path, index_col="Date", parse_dates=True)
            
            if hasattr(df.index, 'tz') and df.index.tz is not None:
                df.index = df.index.tz_localize(None)
            
            start_filter = start_date.date() if isinstance(start_date, datetime) else start_date
            end_filter = end_date.date() if isinstance(end_date, datetime) else end_date
            
            df = df[(df.index.date >= start_filter) & (df.index.date <= end_filter)]
            
            if df.empty:
                return ticker, None, 
