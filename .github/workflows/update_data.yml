name: Update CSVs daily

on:
  schedule:
    # Horario de invierno (CET): 22:00 Espa√±a = 21:00 UTC
    - cron: "0 21 * * 1-5"
    # Horario de verano (CEST): 22:00 Espa√±a = 20:00 UTC  
    - cron: "0 20 * * 1-5"
  workflow_dispatch:

permissions:
  contents: write

jobs:
  update-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install yfinance pandas requests lxml html5lib pytz

      - name: Create update script
        run: |
          cat > update_csv.py << 'EOF'
          import yfinance as yf
          import pandas as pd
          import os
          import glob
          from datetime import datetime, timedelta
          import pytz
          import warnings
          warnings.filterwarnings('ignore')
          
          def normalize_adjusted_dataframe(df):
              """
              Normaliza el DataFrame con datos ajustados (auto_adjust=True)
              Cuando auto_adjust=True, yfinance devuelve: Open, High, Low, Close, Volume
              (NO incluye 'Adj Close' porque todos los valores YA est√°n ajustados)
              """
              # Columnas esperadas cuando auto_adjust=True
              expected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
              
              # Si hay columnas num√©ricas o desordenadas, mapearlas
              if any(isinstance(col, int) for col in df.columns) or len(df.columns) > 5:
                  # Intentar identificar columnas por posici√≥n
                  new_columns = {}
                  col_list = list(df.columns)
                  
                  for i, col in enumerate(col_list):
                      if i < len(expected_columns):
                          new_columns[col] = expected_columns[i]
                  
                  if new_columns:
                      df = df.rename(columns=new_columns)
              
              # Asegurar que solo tenemos las columnas esperadas
              valid_cols = [col for col in expected_columns if col in df.columns]
              df = df[valid_cols]
              
              # Agregar columnas faltantes con NaN
              for col in expected_columns:
                  if col not in df.columns:
                      df[col] = pd.NA
              
              # Reordenar columnas
              df = df[expected_columns]
              
              # Eliminar filas completamente vac√≠as
              df = df.dropna(how='all')
              
              # Eliminar filas donde todos los precios son NaN
              price_cols = ['Open', 'High', 'Low', 'Close']
              df = df.dropna(subset=price_cols, how='all')
              
              return df
          
          def get_existing_tickers():
              """Obtiene solo los tickers que existen en la carpeta data/"""
              csv_files = glob.glob("data/*.csv")
              tickers = []
              for file_path in csv_files:
                  filename = os.path.basename(file_path)
                  if filename.endswith('.csv'):
                      ticker = filename.replace('.csv', '')
                      tickers.append(ticker)
              return sorted(list(set(tickers)))
          
          def regenerate_full_history(ticker):
              """
              Regenera el historial completo de un ticker con auto_adjust=True
              """
              try:
                  print(f"  üîÑ Regenerando historial completo para {ticker}...")
                  
                  yticker = yf.Ticker(ticker)
                  # Descargar TODO el historial con auto_adjust=True
                  data = yticker.history(period="max", auto_adjust=True)
                  
                  if data.empty:
                      print(f"    ‚ùå No se pudo obtener datos")
                      return None
                  
                  # Normalizar DataFrame
                  data = normalize_adjusted_dataframe(data)
                  
                  print(f"    ‚úÖ Descargados {len(data)} d√≠as de historial ajustado")
                  return data
                  
              except Exception as e:
                  print(f"    ‚ùå Error: {str(e)[:100]}")
                  return None
          
          def update_ticker_data(ticker, force_regenerate=False):
              """
              Actualiza datos de un ticker espec√≠fico
              """
              try:
                  filename = f"data/{ticker}.csv"
                  
                  if not os.path.exists(filename) and not force_regenerate:
                      print(f"‚ùå {ticker}: Archivo no encontrado")
                      return False
                  
                  # Verificar si necesitamos regenerar todo el historial
                  needs_full_regeneration = force_regenerate
                  
                  if not needs_full_regeneration and os.path.exists(filename):
                      # Leer archivo existente
                      try:
                          df_existing = pd.read_csv(filename, index_col="Date", parse_dates=True)
                          
                          # Verificar si tiene Adj Close (datos viejos sin ajustar)
                          if 'Adj Close' in df_existing.columns:
                              print(f"‚ö†Ô∏è {ticker}: Detectados datos sin ajustar (tiene 'Adj Close')")
                              needs_full_regeneration = True
                          # Verificar si tiene m√°s de 5 columnas principales
                          elif len([c for c in df_existing.columns if c in ['Open','High','Low','Close','Volume']]) != 5:
                              print(f"‚ö†Ô∏è {ticker}: Estructura de columnas incorrecta")
                              needs_full_regeneration = True
                          
                      except Exception as e:
                          print(f"‚ö†Ô∏è {ticker}: Error leyendo archivo: {e}")
                          needs_full_regeneration = True
                  
                  # Si necesita regeneraci√≥n completa
                  if needs_full_regeneration:
                      data = regenerate_full_history(ticker)
                      if data is not None:
                          data.to_csv(filename)
                          print(f"‚úÖ {ticker}: Historial regenerado y guardado con datos ajustados")
                          return True
                      else:
                          return False
                  
                  # Si no necesita regeneraci√≥n, solo actualizar datos nuevos
                  df_existing = pd.read_csv(filename, index_col="Date", parse_dates=True)
                  df_existing = normalize_adjusted_dataframe(df_existing)
                  
                  if len(df_existing) > 0:
                      last_date = df_existing.index[-1]
                      today = datetime.now()
                      
                      # Verificar si es fin de semana
                      if today.weekday() >= 5:
                          print(f"üìÖ {ticker}: Fin de semana, sin actualizaciones")
                          return True
                      
                      # Si los datos son de hoy, est√° actualizado
                      if last_date.date() >= today.date():
                          print(f"‚úÖ {ticker}: Ya actualizado para hoy {last_date.date()}")
                          return True
                      
                      # Verificar horario del mercado
                      est = pytz.timezone('US/Eastern')
                      now_est = datetime.now(est)
                      
                      if now_est.hour < 17:  # Antes de 5PM EST
                          yesterday = today - timedelta(days=1)
                          while yesterday.weekday() > 4:
                              yesterday = yesterday - timedelta(days=1)
                          
                          if last_date.date() >= yesterday.date():
                              print(f"‚è∞ {ticker}: Datos de {last_date.date()} son los m√°s recientes")
                              return True
                      
                      # Descargar datos nuevos con auto_adjust=True
                      print(f"üì• {ticker}: Actualizando desde {last_date.date()}...")
                      
                      yticker = yf.Ticker(ticker)
                      # Usar per√≠odo corto para actualizaciones, con auto_adjust=True
                      new_data = yticker.history(period="1mo", auto_adjust=True)
                      
                      if not new_data.empty:
                          new_data = normalize_adjusted_dataframe(new_data)
                          new_data.index = pd.to_datetime(new_data.index)
                          
                          # Filtrar solo datos nuevos
                          new_data = new_data[new_data.index > last_date]
                          
                          if len(new_data) > 0:
                              # Combinar con datos existentes
                              combined = pd.concat([df_existing, new_data])
                              combined = combined[~combined.index.duplicated(keep='last')]
                              combined = combined.sort_index()
                              
                              # Guardar
                              combined.to_csv(filename)
                              print(f"‚úÖ {ticker}: +{len(new_data)} d√≠as nuevos (hasta {combined.index[-1].date()})")
                              return True
                          else:
                              print(f"‚ÑπÔ∏è {ticker}: Sin datos nuevos")
                              return True
                      else:
                          print(f"‚ö†Ô∏è {ticker}: Yahoo no devolvi√≥ datos")
                          return True
                  else:
                      # Archivo vac√≠o, descargar todo
                      data = regenerate_full_history(ticker)
                      if data is not None:
                          data.to_csv(filename)
                          return True
                      return False
                      
              except Exception as e:
                  print(f"‚ùå {ticker}: Error - {str(e)[:200]}")
                  return False
          
          def check_and_fix_all_csvs():
              """
              Verifica todos los CSVs y marca los que necesitan regeneraci√≥n
              """
              print("\nüîç VERIFICANDO ESTRUCTURA DE CSVs")
              print("=" * 50)
              
              csv_files = glob.glob("data/*.csv")
              need_fix = []
              ok_files = []
              
              for file_path in csv_files:
                  ticker = os.path.basename(file_path).replace('.csv', '')
                  try:
                      df = pd.read_csv(file_path, index_col="Date", parse_dates=True)
                      
                      # Verificar si tiene Adj Close (indica datos NO ajustados)
                      if 'Adj Close' in df.columns:
                          need_fix.append(ticker)
                          print(f"  ‚ö†Ô∏è {ticker}: Tiene 'Adj Close' - necesita regeneraci√≥n")
                      # Verificar columnas num√©ricas o estructura incorrecta
                      elif any(isinstance(col, int) for col in df.columns):
                          need_fix.append(ticker)
                          print(f"  ‚ö†Ô∏è {ticker}: Columnas num√©ricas - necesita regeneraci√≥n")
                      elif len(df.columns) > 5:
                          need_fix.append(ticker)
                          print(f"  ‚ö†Ô∏è {ticker}: Demasiadas columnas ({len(df.columns)}) - necesita regeneraci√≥n")
                      else:
                          ok_files.append(ticker)
                      
                  except Exception as e:
                      need_fix.append(ticker)
                      print(f"  ‚ùå {ticker}: Error leyendo - necesita regeneraci√≥n")
              
              print(f"\nüìä Resumen:")
              print(f"  ‚úÖ Archivos OK: {len(ok_files)}")
              print(f"  ‚ö†Ô∏è Necesitan regeneraci√≥n: {len(need_fix)}")
              
              return need_fix
          
          if __name__ == "__main__":
              print("=" * 60)
              print("üöÄ ACTUALIZACI√ìN DIARIA CON DATOS AJUSTADOS")
              print("=" * 60)
              print(f"üìÖ Fecha/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC")
              print("‚ÑπÔ∏è Modo: auto_adjust=True (datos ajustados por dividendos y splits)")
              
              # Verificar qu√© CSVs necesitan regeneraci√≥n
              tickers_to_regenerate = check_and_fix_all_csvs()
              
              if tickers_to_regenerate:
                  print(f"\nüîß REGENERANDO {len(tickers_to_regenerate)} ARCHIVOS")
                  print("=" * 60)
                  
                  regenerated = 0
                  failed = 0
                  
                  # Priorizar tickers importantes
                  priority = ['SPY', 'QQQ', 'AAPL', 'MSFT', 'GOOGL']
                  priority_regen = [t for t in priority if t in tickers_to_regenerate]
                  other_regen = [t for t in tickers_to_regenerate if t not in priority]
                  
                  for i, ticker in enumerate(priority_regen + other_regen, 1):
                      print(f"\n[{i}/{len(tickers_to_regenerate)}] Regenerando {ticker}...")
                      if update_ticker_data(ticker, force_regenerate=True):
                          regenerated += 1
                      else:
                          failed += 1
                      
                      # Pausa cada 20 tickers
                      if i % 20 == 0:
                          import time
                          time.sleep(1)
                  
                  print(f"\n‚úÖ Regenerados: {regenerated}")
                  print(f"‚ùå Fallidos: {failed}")
              
              print("\n" + "=" * 60)
              print("üìä ACTUALIZANDO DATOS RECIENTES")
              print("=" * 60)
              
              # Ahora actualizar todos los tickers
              all_tickers = get_existing_tickers()
              print(f"\nüìÅ Total de tickers a verificar: {len(all_tickers)}")
              
              # Priorizar tickers importantes
              priority_tickers = ['SPY', 'QQQ', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA']
              priority_found = [t for t in priority_tickers if t in all_tickers]
              other_tickers = [t for t in all_tickers if t not in priority_tickers]
              
              tickers_ordered = priority_found + other_tickers
              
              updated = 0
              no_changes = 0
              errors = 0
              
              for i, ticker in enumerate(tickers_ordered, 1):
                  if i % 10 == 1:  # Cada 10 tickers
                      print(f"\n--- Procesando tickers {i} a {min(i+9, len(tickers_ordered))} de {len(tickers_ordered)} ---")
                  
                  # No regenerar si ya lo hicimos arriba
                  if ticker not in tickers_to_regenerate:
                      result = update_ticker_data(ticker, force_regenerate=False)
                      if result:
                          updated += 1
                      else:
                          errors += 1
                  
                  # Pausa cada 50 tickers
                  if i % 50 == 0:
                      import time
                      time.sleep(2)
              
              # Resumen final
              print("\n" + "=" * 60)
              print("üìä RESUMEN FINAL")
              print("=" * 60)
              print(f"‚úÖ Actualizados/Verificados: {updated + regenerated}")
              print(f"‚ùå Errores: {errors + failed}")
              print(f"üìä Total procesados: {len(tickers_ordered)}")
              
              # Verificaci√≥n final
              print("\nüîç Verificaci√≥n final (primeros 5 tickers):")
              for ticker in priority_found[:5]:
                  try:
                      df = pd.read_csv(f"data/{ticker}.csv", index_col="Date", parse_dates=True)
                      has_adj = 'Adj Close' in df.columns
                      status = "‚ö†Ô∏è TIENE ADJ CLOSE" if has_adj else "‚úÖ OK (ajustado)"
                      print(f"  {ticker}: {len(df)} registros, √∫ltimo: {df.index[-1].date()} - {status}")
                  except Exception as e:
                      print(f"  {ticker}: Error - {e}")
              
              print("\n‚úÖ Proceso completado")
              print("=" * 60)
          EOF

      - name: Run update script
        run: python update_csv.py

      - name: Final verification
        run: |
          python -c "
          import pandas as pd
          import glob
          
          print('üîç Verificaci√≥n final de estructura...')
          
          samples = glob.glob('data/*.csv')[:10]
          issues = []
          
          for file in samples:
              try:
                  df = pd.read_csv(file, index_col='Date', parse_dates=True)
                  ticker = file.split('/')[-1].replace('.csv', '')
                  
                  if 'Adj Close' in df.columns:
                      issues.append(f'{ticker}: TIENE Adj Close (no ajustado)')
                  elif len(df.columns) != 5:
                      issues.append(f'{ticker}: {len(df.columns)} columnas (esperadas 5)')
                  else:
                      print(f'  ‚úÖ {ticker}: Estructura correcta (sin Adj Close)')
              except Exception as e:
                  issues.append(f'{file}: {e}')
          
          if issues:
              print('\\n‚ö†Ô∏è Problemas detectados:')
              for issue in issues:
                  print(f'  - {issue}')
          else:
              print('\\n‚úÖ Todos los archivos verificados tienen estructura correcta')
          "

      - name: Commit and push changes
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          git add data/*.csv
          
          if git diff --staged --quiet; then
            echo "üìä No hay cambios para commitear"
          else
            COMMIT_MSG="üìä Update CSVs $(date -u +'%Y-%m-%d') - All data adjusted (auto_adjust=True)"
            git commit -m "$COMMIT_MSG"
            git push
            echo "‚úÖ Cambios pusheados exitosamente"
          fi
