name: Update CSVs daily

on:
  schedule:
    # Ejecutar de lunes a viernes a las 21:00 UTC (22:00 CET)
    - cron: "0 21 * * 1-5"
  workflow_dispatch:
    inputs:
      force_update:
        description: 'Forzar actualizaci√≥n incluso en fin de semana'
        required: false
        default: false
        type: boolean

permissions:
  contents: write

jobs:
  update-data:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          persist-credentials: true
          fetch-depth: 0  # Importante para el historial completo

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install yfinance pandas requests lxml html5lib pytz

      - name: Create update script
        run: |
          cat > update_csv.py << 'EOF'
          import sys
          import yfinance as yf
          import pandas as pd
          import os
          import glob
          from datetime import datetime, timedelta
          import pytz
          import warnings
          warnings.filterwarnings('ignore')
          
          FORCE_UPDATE = '--force' in sys.argv
          
          def normalize_adjusted_dataframe(df):
              """Normaliza el DataFrame con datos ajustados"""
              expected_columns = ['Open', 'High', 'Low', 'Close', 'Volume']
              
              if any(isinstance(col, int) for col in df.columns) or len(df.columns) > 5:
                  new_columns = {}
                  col_list = list(df.columns)
                  
                  for i, col in enumerate(col_list):
                      if i < len(expected_columns):
                          new_columns[col] = expected_columns[i]
                  
                  if new_columns:
                      df = df.rename(columns=new_columns)
              
              valid_cols = [col for col in expected_columns if col in df.columns]
              df = df[valid_cols]
              
              for col in expected_columns:
                  if col not in df.columns:
                      df[col] = pd.NA
              
              df = df[expected_columns]
              df = df.dropna(how='all')
              
              price_cols = ['Open', 'High', 'Low', 'Close']
              df = df.dropna(subset=price_cols, how='all')
              
              return df
          
          def get_existing_tickers():
              """Obtiene tickers existentes"""
              csv_files = glob.glob("data/*.csv")
              tickers = []
              for file_path in csv_files:
                  filename = os.path.basename(file_path)
                  if filename.endswith('.csv'):
                      ticker = filename.replace('.csv', '')
                      tickers.append(ticker)
              return sorted(list(set(tickers)))
          
          def regenerate_full_history(ticker):
              """Regenera historial completo"""
              try:
                  print(f"  üîÑ Regenerando {ticker}...")
                  
                  yticker = yf.Ticker(ticker)
                  data = yticker.history(period="max", auto_adjust=True)
                  
                  if data.empty:
                      print(f"    ‚ùå No se pudo obtener datos")
                      return None
                  
                  data = normalize_adjusted_dataframe(data)
                  print(f"    ‚úÖ {len(data)} d√≠as descargados")
                  return data
                  
              except Exception as e:
                  print(f"    ‚ùå Error: {str(e)[:100]}")
                  return None
          
          def update_ticker_data(ticker, force_regenerate=False):
              """Actualiza datos de ticker"""
              try:
                  filename = f"data/{ticker}.csv"
                  
                  if not os.path.exists(filename) and not force_regenerate:
                      print(f"‚ùå {ticker}: Archivo no encontrado")
                      return False
                  
                  needs_full_regeneration = force_regenerate
                  
                  if not needs_full_regeneration and os.path.exists(filename):
                      try:
                          df_existing = pd.read_csv(filename, index_col="Date", parse_dates=True)
                          
                          if 'Adj Close' in df_existing.columns:
                              print(f"‚ö†Ô∏è {ticker}: Datos no ajustados detectados")
                              needs_full_regeneration = True
                          elif len([c for c in df_existing.columns if c in ['Open','High','Low','Close','Volume']]) != 5:
                              print(f"‚ö†Ô∏è {ticker}: Estructura incorrecta")
                              needs_full_regeneration = True
                          
                      except Exception as e:
                          print(f"‚ö†Ô∏è {ticker}: Error leyendo archivo")
                          needs_full_regeneration = True
                  
                  if needs_full_regeneration:
                      data = regenerate_full_history(ticker)
                      if data is not None:
                          data.to_csv(filename)
                          print(f"‚úÖ {ticker}: Regenerado")
                          return True
                      else:
                          return False
                  
                  # Actualizaci√≥n incremental
                  df_existing = pd.read_csv(filename, index_col="Date", parse_dates=True)
                  df_existing = normalize_adjusted_dataframe(df_existing)
                  
                  if len(df_existing) > 0:
                      last_date = df_existing.index[-1]
                      today = datetime.now()
                      
                      if not FORCE_UPDATE and today.weekday() >= 5:
                          print(f"üìÖ {ticker}: Fin de semana")
                          return True
                      
                      if last_date.date() >= today.date():
                          print(f"‚úÖ {ticker}: Actualizado")
                          return True
                      
                      est = pytz.timezone('US/Eastern')
                      now_est = datetime.now(est)
                      
                      if not FORCE_UPDATE and now_est.hour < 17:
                          yesterday = today - timedelta(days=1)
                          while yesterday.weekday() > 4:
                              yesterday = yesterday - timedelta(days=1)
                          
                          if last_date.date() >= yesterday.date():
                              print(f"‚è∞ {ticker}: Datos recientes")
                              return True
                      
                      print(f"üì• {ticker}: Actualizando...")
                      
                      yticker = yf.Ticker(ticker)
                      new_data = yticker.history(period="1mo", auto_adjust=True)
                      
                      if not new_data.empty:
                          new_data = normalize_adjusted_dataframe(new_data)
                          new_data.index = pd.to_datetime(new_data.index)
                          
                          new_data = new_data[new_data.index > last_date]
                          
                          if len(new_data) > 0:
                              combined = pd.concat([df_existing, new_data])
                              combined = combined[~combined.index.duplicated(keep='last')]
                              combined = combined.sort_index()
                              
                              combined.to_csv(filename)
                              print(f"‚úÖ {ticker}: +{len(new_data)} d√≠as")
                              return True
                          else:
                              print(f"‚ÑπÔ∏è {ticker}: Sin datos nuevos")
                              return True
                      else:
                          print(f"‚ö†Ô∏è {ticker}: Sin respuesta de Yahoo")
                          return True
                  else:
                      data = regenerate_full_history(ticker)
                      if data is not None:
                          data.to_csv(filename)
                          return True
                      return False
                      
              except Exception as e:
                  print(f"‚ùå {ticker}: Error - {str(e)[:200]}")
                  return False
          
          def check_and_fix_all_csvs():
              """Verifica CSVs que necesitan regeneraci√≥n"""
              print("\nüîç VERIFICANDO CSVs")
              print("=" * 50)
              
              csv_files = glob.glob("data/*.csv")
              need_fix = []
              ok_files = []
              
              for file_path in csv_files:
                  ticker = os.path.basename(file_path).replace('.csv', '')
                  try:
                      df = pd.read_csv(file_path, index_col="Date", parse_dates=True)
                      
                      if 'Adj Close' in df.columns:
                          need_fix.append(ticker)
                          print(f"  ‚ö†Ô∏è {ticker}: Necesita regeneraci√≥n")
                      elif any(isinstance(col, int) for col in df.columns):
                          need_fix.append(ticker)
                          print(f"  ‚ö†Ô∏è {ticker}: Columnas num√©ricas")
                      elif len(df.columns) > 5:
                          need_fix.append(ticker)
                          print(f"  ‚ö†Ô∏è {ticker}: Demasiadas columnas")
                      else:
                          ok_files.append(ticker)
                      
                  except Exception as e:
                      need_fix.append(ticker)
                      print(f"  ‚ùå {ticker}: Error leyendo")
              
              print(f"\nüìä Resumen:")
              print(f"  ‚úÖ OK: {len(ok_files)}")
              print(f"  ‚ö†Ô∏è Necesitan fix: {len(need_fix)}")
              
              return need_fix
          
          if __name__ == "__main__":
              print("=" * 60)
              print("üöÄ ACTUALIZACI√ìN DIARIA")
              print("=" * 60)
              print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} UTC")
              if FORCE_UPDATE:
                  print("‚ö° MODO FORCE ACTIVADO")
              
              # INICIALIZAR VARIABLES DE CONTEO
              regenerated = 0
              failed = 0
              updated = 0
              errors = 0
              
              tickers_to_regenerate = check_and_fix_all_csvs()
              
              if tickers_to_regenerate:
                  print(f"\nüîß REGENERANDO {len(tickers_to_regenerate)} ARCHIVOS")
                  print("=" * 60)
                  
                  priority = ['SPY', 'QQQ', 'AAPL', 'MSFT', 'GOOGL']
                  priority_regen = [t for t in priority if t in tickers_to_regenerate]
                  other_regen = [t for t in tickers_to_regenerate if t not in priority]
                  
                  for i, ticker in enumerate(priority_regen + other_regen, 1):
                      print(f"\n[{i}/{len(tickers_to_regenerate)}] {ticker}")
                      if update_ticker_data(ticker, force_regenerate=True):
                          regenerated += 1
                      else:
                          failed += 1
                      
                      if i % 20 == 0:
                          import time
                          time.sleep(1)
                  
                  print(f"\n‚úÖ Regenerados: {regenerated}")
                  print(f"‚ùå Fallidos: {failed}")
              else:
                  print("\n‚úÖ Todos los CSVs est√°n en formato correcto")
              
              print("\nüìä ACTUALIZANDO DATOS")
              print("=" * 60)
              
              all_tickers = get_existing_tickers()
              print(f"\nüìÅ Total: {len(all_tickers)}")
              
              priority_tickers = ['SPY', 'QQQ', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA']
              priority_found = [t for t in priority_tickers if t in all_tickers]
              other_tickers = [t for t in all_tickers if t not in priority_tickers]
              
              tickers_ordered = priority_found + other_tickers
              
              for i, ticker in enumerate(tickers_ordered, 1):
                  if i % 10 == 1:
                      print(f"\n--- {i} a {min(i+9, len(tickers_ordered))} de {len(tickers_ordered)} ---")
                  
                  if ticker not in tickers_to_regenerate:
                      result = update_ticker_data(ticker, force_regenerate=False)
                      if result:
                          updated += 1
                      else:
                          errors += 1
                  
                  if i % 50 == 0:
                      import time
                      time.sleep(2)
              
              print("\nüìä RESUMEN FINAL")
              print("=" * 60)
              print(f"‚úÖ Procesados: {updated + regenerated}")
              print(f"‚ùå Errores: {errors + failed}")
              print(f"üìä Total: {len(tickers_ordered)}")
              
              print("\nüîç Verificaci√≥n:")
              for ticker in priority_found[:5]:
                  try:
                      df = pd.read_csv(f"data/{ticker}.csv", index_col="Date", parse_dates=True)
                      has_adj = 'Adj Close' in df.columns
                      status = "‚ö†Ô∏è TIENE ADJ CLOSE" if has_adj else "‚úÖ OK"
                      print(f"  {ticker}: {len(df)} registros - {status}")
                  except Exception as e:
                      print(f"  {ticker}: Error")
              
              print("\n‚úÖ Proceso completado")
          EOF

      - name: Run update script
        run: |
          if [[ "${{ github.event.inputs.force_update }}" == "true" ]]; then
            python update_csv.py --force
          else
            python update_csv.py
          fi

      - name: Clean up delisted tickers
        run: |
          python -c "
          import os
          import glob
          import pandas as pd
          
          # Lista de tickers que sabemos est√°n dados de baja
          delisted_tickers = ['ABK', 'ANR', 'AYE', 'BMC', 'CBE', 'COV', 'EVHC', 'FMCN', 'GAS', 'MEE', 'NCC', 'NFX', 'SBL', 'VIP', 'VMED']
          
          print('üßπ Limpiando tickers dados de baja...')
          
          removed_count = 0
          for ticker in delisted_tickers:
              file_path = f'data/{ticker}.csv'
              if os.path.exists(file_path):
                  try:
                      os.remove(file_path)
                      print(f'  ‚ùå Eliminado: {ticker}.csv')
                      removed_count += 1
                  except Exception as e:
                      print(f'  ‚ö†Ô∏è Error eliminando {ticker}: {e}')
          
          print(f'\\n‚úÖ Eliminados {removed_count} archivos de tickers dados de baja')
          
          # Verificar cu√°ntos archivos quedan
          remaining_files = len(glob.glob('data/*.csv'))
          print(f'üìÅ Archivos restantes: {remaining_files}')
          "

      - name: Final verification
        run: |
          python -c "
          import pandas as pd
          import glob
          
          print('üîç Verificaci√≥n final...')
          
          samples = glob.glob('data/*.csv')[:10]
          issues = []
          
          for file in samples:
              try:
                  df = pd.read_csv(file, index_col='Date', parse_dates=True)
                  ticker = file.split('/')[-1].replace('.csv', '')
                  
                  if 'Adj Close' in df.columns:
                      issues.append(f'{ticker}: TIENE Adj Close')
                  elif len(df.columns) != 5:
                      issues.append(f'{ticker}: {len(df.columns)} columnas')
                  else:
                      print(f'  ‚úÖ {ticker}: OK')
              except Exception as e:
                  issues.append(f'{file}: {e}')
          
          if issues:
              print('\\n‚ö†Ô∏è Problemas:')
              for issue in issues:
                  print(f'  - {issue}')
          else:
              print('\\n‚úÖ Todos los archivos OK')
          "

      - name: Commit and push changes
        run: |
          # Configurar git
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          
          # IMPORTANTE: Hacer pull primero para obtener cambios remotos
          echo "üì• Obteniendo cambios remotos..."
          git pull origin main --rebase || {
              echo "‚ö†Ô∏è Error en pull, intentando merge..."
              git rebase --abort 2>/dev/null || true
              git pull origin main --no-rebase --strategy=ours || {
                  echo "‚ùå No se pudo hacer pull, abortando..."
                  exit 1
              }
          }
          
          # A√±adir todos los cambios
          git add .
          
          # Verificar si hay cambios staged
          if ! git diff --staged --quiet; then
            echo "üìä Hay cambios para commitear, procediendo..."
            
            # Commit
            COMMIT_MSG="üìä Update CSVs $(date -u +'%Y-%m-%d %H:%M') UTC - Auto update"
            git commit -m "$COMMIT_MSG"
            
            # Push con reintentos
            MAX_RETRIES=3
            RETRY_COUNT=0
            
            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if git push origin main; then
                echo "‚úÖ Cambios pusheados exitosamente"
                break
              else
                RETRY_COUNT=$((RETRY_COUNT + 1))
                echo "‚ö†Ô∏è Push fallido, intento $RETRY_COUNT de $MAX_RETRIES"
                
                if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                  echo "üì• Haciendo pull antes de reintentar..."
                  git pull origin main --rebase
                  sleep 2
                else
                  echo "‚ùå No se pudo hacer push despu√©s de $MAX_RETRIES intentos"
                  exit 1
                fi
              fi
            done
          else
            echo "üìä No hay cambios para commitear"
          fi

      - name: Summary
        if: always()
        run: |
          echo "üìä RESUMEN DE LA EJECUCI√ìN"
          echo "=========================="
          echo "Fecha: $(date -u +'%Y-%m-%d %H:%M:%S') UTC"
          echo "Archivos CSV: $(ls -1 data/*.csv 2>/dev/null | wc -l)"
          echo ""
          
          # Mostrar √∫ltimos commits
          echo "üìù √öltimos 3 commits:"
          git log --oneline -3
